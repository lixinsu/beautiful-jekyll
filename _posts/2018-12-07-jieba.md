---
layout: post
published: false
title: jieba 源码解读
---

#  阅读jieba源码总结
结巴分词采用的是最短路分词结果HMM的方法
- 首先利用正则表达式将文字区域找到
- 首先利用词典构建DAG
- 将分词问题转化成求最短路的问题
- 对于连续的单个字采用HMM进行处理

我们从其中抽取去其关键的函数在notebook中运行

## 首先利用正则表达式将找文字区域


```python
import re
```


```python

re_han_default = re.compile("([\u4E00-\u9FD5a-zA-Z0-9+#&\._%\-]+)", re.U)
re_han_cut_all = re.compile("([\u4E00-\u9FD5]+)", re.U)
blocks = re_han_default.split("计算所的学生666啊，科研为国分忧！")
blocks
```




    ["", "计算所的学生666啊", "，", "科研为国分忧", "！"]



## 加载词典
每个词出现的频次存储在lfreq中，词出现的总数ltotal


```python
def gen_pfdict(f):
        lfreq = {}
        ltotal = 0
        f_name = f
        for lineno, line in enumerate(open(f), 1):
            try:
                line = line.strip()
                word, freq = line.split(" ")[:2]
                freq = int(freq)
                lfreq[word] = freq
                ltotal += freq
                for ch in range(len(word)):
                    wfrag = word[:ch + 1]
                    if wfrag not in lfreq:
                        lfreq[wfrag] = 0
            except ValueError:
                raise ValueError(
                    "invalid dictionary entry in %s at Line %s: %s" % (f_name, lineno, line))
        
        return lfreq, ltotal
lfreq, ltotal = gen_pfdict("jieba/dict.txt")
print("vocab size %s" % len(lfreq))
print(ltotal)
```

    vocab size 498113
    60101967


## 根据词典生成DAG
任意片段出现在词典中，则连一条其实字到结束字的表


```python
def get_DAG(sentence, lfreq):
        DAG = {}
        N = len(sentence)
        for k in range(N):
            tmplist = []
            i = k
            frag = sentence[k]
            while i < N and frag in lfreq:
                if lfreq[frag]:
                    tmplist.append(i)
                i += 1
                frag = sentence[k:i + 1]
            if not tmplist:
                tmplist.append(k)
            DAG[k] = tmplist
        return DAG
dag = get_DAG("计算所的学生", lfreq)
print(dag)
```

    {0: [0, 1, 2], 1: [1], 2: [2], 3: [3], 4: [4, 5], 5: [5]}


## search 场景切词
这种模式就是存在于词典中的片段都返回，词是存在重叠的


```python
def cut_all( sentence, freq):
        dag = get_DAG(sentence, freq)
        old_j = -1
        for k, L in dag.items():
            if len(L) == 1 and k > old_j:
                yield sentence[k:L[0] + 1]
                old_j = L[0]
            else:
                for j in L:
                    if j > k:
                        yield sentence[k:j + 1]
                        old_j = j
list(cut_all("计算所的学生 hello world", lfreq))
```




    ["计算",
     "计算所",
     "的",
     "学生",
     " ",
     "h",
     "e",
     "l",
     "l",
     "o",
     " ",
     "w",
     "o",
     "r",
     "l",
     "d"]



## 计算最短路
这里路径上是有权重的，即词频


```python
import math
def calc(sentence, DAG, route):
        log = math.log
        N = len(sentence)
        route[N] = (0, 0)
        logtotal = log(ltotal)
        for idx in range(N - 1, -1, -1):
            route[idx] = max((log(lfreq.get(sentence[idx:x + 1]) or 1) -
                              logtotal + route[x + 1][0], x) for x in DAG[idx])

```


```python
route = {}
calc("计算所的学生",dag,route)
route
```




    {0: (-28.242864647150427, 2),
     1: (-28.30231792899792, 1),
     2: (-19.483222132562165, 2),
     3: (-12.816218169183207, 3),
     4: (-7.5770626833230885, 5),
     5: (-7.886777578011891, 5),
     6: (0, 0)}



## 调用最短路切词
对于连续单个数字或者英文合并一下


```python
re_eng = re.compile("[a-zA-Z0-9]", re.U)

def cut_DAG_NO_HMM(sentence, freq):
    DAG = get_DAG(sentence, freq)
    route = {}
    calc(sentence, DAG, route)
    x = 0
    N = len(sentence)
    buf = ""
    while x < N:
        y = route[x][1] + 1
        l_word = sentence[x:y]
        if re_eng.match(l_word) and len(l_word) == 1:
            buf += l_word
            x = y
        else:
            if buf:
                yield buf
                buf = ""
            yield l_word
            x = y
    if buf:
        yield buf
        buf = ""
list(cut_DAG_NO_HMM("计算所的学生 hello world", lfreq))      
```




    ["计算所", "的", "学生", " ", "hello", " ", "world"]



## 调用对于连续单字调用HMM识别


```python
import ipdb
def cut_DAG(sentence, freq):
    DAG = get_DAG(sentence, freq)
    route = {}
    calc(sentence, DAG, route)
    x = 0
    buf = ""
    N = len(sentence)
    while x < N:
        y = route[x][1] + 1
        l_word = sentence[x:y]
        if y - x == 1:
            buf += l_word
        else:
            if buf:
                if len(buf) == 1:
                    yield buf
                    buf = ""
                else:
                    if not self.FREQ.get(buf):
                        # recognized = finalseg.cut(buf)
                        recognized = ["f"]
                        for t in recognized:
                            yield t
                    else:
                        for elem in buf:
                            yield elem
                    buf = ""
            yield l_word
        x = y

    if buf:
        if len(buf) == 1:
            yield buf
        elif not self.FREQ.get(buf):
            # recognized = finalseg.cut(buf)
            recognized = []
            for t in recognized:
                yield t
        else:
            for elem in buf:
                yield elem
list(cut_DAG("计算所的学生", lfreq))
```

## 整体算法流程如下


```python
re_han_default = re.compile("([\u4E00-\u9FD5a-zA-Z0-9+#&\._%\-]+)", re.U)

re_skip_default = re.compile("(\r\n|\s)", re.U)
re_han_cut_all = re.compile("([\u4E00-\u9FD5]+)", re.U)
re_skip_cut_all = re.compile("[^a-zA-Z0-9+#\n]", re.U)
def cut(sentence, cut_all=False, HMM=False, freq=None):
    """
    The main function that segments an entire sentence that contains
    Chinese characters into separated words.

    Parameter:
        - sentence: The str(unicode) to be segmented.
        - cut_all: Model type. True for full pattern, False for accurate pattern.
        - HMM: Whether to use the Hidden Markov Model.
    """

    # Choose pattern
    if cut_all:
        re_han = re_han_cut_all
        re_skip = re_skip_cut_all
    else:
        re_han = re_han_default
        re_skip = re_skip_default
    # Choose cut algorithm
    if cut_all:
        cut_block = cut_all
    elif HMM:
        cut_block = cut_DAG
    else:
        cut_block = cut_DAG_NO_HMM

    blocks = re_han.split(sentence)
    for blk in blocks:
        if not blk:
            continue
        if re_han.match(blk):
            for word in cut_block(blk, freq):
                yield word
        else:
            tmp = re_skip.split(blk)
            for x in tmp:
                if re_skip.match(x):
                    yield x
                elif not cut_all:
                    for xx in x:
                        yield xx
                else:
                    yield x
```


```python
list(cut("计算所的学生", freq=lfreq))
```




    ["计算所", "的", "学生"]


